[{"content":"Introduction I was excited to tackle today\u0026rsquo;s topic of using a CI/CD tool, GitHub Actions, to automate part of the deployment of my Notes app. One of my favorite aspects of working with technology is getting to automate things. Much of what I built in my last post was done manually in a graphical interface, which wasn\u0026rsquo;t as exciting for me, but was a necessary step in the learning process. So, let\u0026rsquo;s see how I automated part of the deployment process using GitHub Actions.\nSetup Create workflow in GitHub Actions After looking at some guides, I started by going to the repo for my Notes app and going to the Actions tab, then New Workflow. From there I searched for \u0026ldquo;ECS\u0026rdquo; and selected the one result - Deploy to Amazon ECS.\nI think this was a good place to start, but I realized halfway through that this template was a bit out of date. I ended up having to pull pieces from other specific workflows to update most sections. This Github Docs page along with the official AWS Actions helped me figure out all the modifications I ended up needing to make.\nCreate JSON task definition for ECS The next thing I needed to do was to get the JSON version of the task definition that I created in my last post. I did this by going to ECS in the AWS console, then Task definitions, selecting my task - notes-api-task, selecting the latest revision, and selecting the JSON tab. I now know another way to do this via the AWS CLI:\naws ecs list-task-definitions Which will give you the ARN to use the following command to get the JSON:\naws ecs describe-task-definition --task-definition arn:aws:ecs:us-west-2:\u0026lt;AWS ID\u0026gt;:task-definition/notes-api-task So I copied the JSON and pasted into a file in the root of my notes project folder and called it notes-api-task.json. I also found out later that I needed to modify this task definition.\nAdd service to ECS cluster Next, I needed to stop the existing task running in my ECS cluster and change it to a service. I stopped the existing task by going to the ECS console, then my notes-api cluster, then the Tasks tab, selected the running task, and hit Stop.\nTo create the service, I went to Services tab in my ECS cluster, clicked Create, and changed the following:\nLaunch type: EC2\nTask definition - Family: notes-api-task (created previously)\nService name: notes-api-service\nWith the task definition and service in place, I went back to the GitHub Actions workflow and edited the environment variables it had:\nenv: AWS_REGION: us-west-2  # set this to your preferred AWS region, e.g. us-west-1 ECR_REPOSITORY: notes-api  # set this to your Amazon ECR repository name ECS_SERVICE: notes-api-service  # set this to your Amazon ECS service name ECS_CLUSTER: notes-api  # set this to your Amazon ECS cluster name ECS_TASK_DEFINITION: notes-api-task.json # set this to the path to your Amazon ECS task definition # file, e.g. .aws/task-definition.json CONTAINER_NAME: notes-api-container  # set this to the name of the container in the # containerDefinitions section of your task definition  It looked like I had everything in place, so I made a new commit to the main branch to kick off the workflow and it errored on the first real step - AWS credentials.\nIt seemed obvious as I saw it and from there I went down a bit of a rabbit hole of methods and best practices for authorizing GitHub to talk to AWS.\nConnect GitHub to AWS Add identity provider to AWS After some reading and consideration, I decided to use the OpenID Connect (OIDC) method for authentication as it seemed to be the best practice as well as being simple enough to learn in one sitting. I started by going to the IAM console in AWS, then Indentity providers, and clicked Add provider.\nI then entered the following:\n Provider type: OpenID Connect Provider URL: token.actions.githubusercontent.com Audience sts.amazonaws.com  Create role for GitHub Actions Then I added a role for GitHub Actions by going to Roles within IAM, then Create role. I selected Custom trust policy and added the following JSON:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Federated\u0026#34;: \u0026#34;arn:aws:iam::\u0026lt;AWS ID\u0026gt;:oidc-provider/token.actions.githubusercontent.com\u0026#34; }, \u0026#34;Condition\u0026#34;: { \u0026#34;StringLike\u0026#34;: { \u0026#34;token.actions.githubusercontent.com:sub\u0026#34;: \u0026#34;repo:rickthackeray/notes:*\u0026#34; }, \u0026#34;StringEquals\u0026#34;: { \u0026#34;token.actions.githubusercontent.com:aud\u0026#34;: \u0026#34;sts.amazonaws.com\u0026#34; } } } ] } With that, I pushed a commit to kick off a new instance of the workflow and saw success! Well, I got a check mark for the first section: \u0026ldquo;Configure AWS credentials\u0026rdquo; - it failed on the very next one: \u0026ldquo;Login to Amazon ECR\u0026rdquo;.\nAdd permission to authenticate with ECR To remedy this, I went to the IAM console, then Roles, and selected my \u0026ldquo;github-actions\u0026rdquo; role. From there I went to Permissions, Add Permissions, and Create Inline Policy. I then selected the JSON tab and added this:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;GetAuthorizationToken\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ecr:GetAuthorizationToken\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } I then pushed a new commit to rerun the workflow and got a second check mark! Of course it failed on the step after that.\nBuild, tag, and push Docker image to ECR I was at 2/5 for successful steps in my workflow. Up to this point I didn\u0026rsquo;t need to modify the workflow template other than inserting the initial configuration settings in the form of environment variables. That was about to change.\nThe error on this step was that it couldn\u0026rsquo;t find the Dockerfile. I realized that my folder structure was the culprit. See, the Dockerfile and the rest of the code for this part of the app was in a subfolder: \u0026ldquo;backend\u0026rdquo;. I fixed this by changing the run command, adding cd backend. The final version of the step looked like this:\n- name: Build, tag, and push docker image to Amazon ECR env: REGISTRY: ${{ steps.login-ecr.outputs.registry }} REPOSITORY: ${{ env.ECR_REPOSITORY }} IMAGE_TAG: ${{ env.IMAGE_TAG }} run: |cd backend docker build -t $REGISTRY/$REPOSITORY:$IMAGE_TAG . docker push $REGISTRY/$REPOSITORY:$IMAGE_TAG I suspect there are better ways to do this and I look forward to learning that one day.\nI could see in the Workflow log that the image was successfully building, but there was still an error on this step that indicated my \u0026ldquo;github-actions\u0026rdquo; role didn\u0026rsquo;t have the right permissions. I eventually fixed it by adding the following inline policy to the role:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowPushPull\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ecr:BatchGetImage\u0026#34;, \u0026#34;ecr:BatchCheckLayerAvailability\u0026#34;, \u0026#34;ecr:CompleteLayerUpload\u0026#34;, \u0026#34;ecr:GetDownloadUrlForLayer\u0026#34;, \u0026#34;ecr:InitiateLayerUpload\u0026#34;, \u0026#34;ecr:PutImage\u0026#34;, \u0026#34;ecr:UploadLayerPart\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:ecr:us-west-2:\u0026lt;AWS ID\u0026gt;:repository/*\u0026#34; } ] } With that in place, I got another check mark - 3/5!\nAnd the next step failed, unsurprisingly. I was getting an error that it couldn\u0026rsquo;t update the task definition with the new image.\nConfigure workflow to update ECS task definition One of the problems seemed to be related to the filename format for the image. The workflow was using the hash of the commit to tag the new build. So, I tried setting it to a static name of \u0026ldquo;Latest\u0026rdquo; and that worked. I\u0026rsquo;m sure there was a way to get the dynamic tagging convention to work, but it wasn\u0026rsquo;t necessary at this point.\nThis didn\u0026rsquo;t get me the next check mark, though. The new error sounded like a permissions issue. I struggled with finding the right specific permissions for ECS that the workflow needed. So, I ended up opening it up fully for ECS. This is not ideal in an environment where security is important as it does not meet the principle of least privilege, but it allowed me to continue learning in my fairly low risk environment. Here is the policy that I added to my github-actions role:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ecs:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } After that, I pushed a new commit and got another check mark - 4/5! It was time to get that last one!\nConfigure workflow to deploy ECS task This one was possibly the most difficult. The workflow logs weren\u0026rsquo;t really giving an error - just showing that the attempt at deploying the ECS task was timing out. It took a lot of troubleshooting effort to find out that the issue was with how I was handling secrets that the app needed.\nYou can see in a previous post that I used the volume feature of Docker to add a keys.env file at image runtime. The problem was that the EC2 instance that ECS was controlling didn\u0026rsquo;t have this file and the task definition didn\u0026rsquo;t have the volume parameter for the docker run command configured.\nTo get the keys.env file onto the EC2 instance, I simply used scp to copy it over. I\u0026rsquo;m guessing this isn\u0026rsquo;t how this is typically done, as I can see issues coming up when working with a team or at larger scales, but again it allowed me to continue learning more instead of fixating on something with lower return. The phrase \u0026ldquo;Don\u0026rsquo;t let perfect be the enemy of good\u0026rdquo; comes to mind.\nAnyway, I could tell by ssh\u0026rsquo;ing into the EC2 instance and manually running the container that this worked, but I still needed to update the task definition to include the file as a Docker volume at runtime. I did that by modifying the notes-api-task.json file I created before, adding mountPoints and volumes sections. Here is what the file ended up as:\n{ \u0026#34;taskDefinitionArn\u0026#34;: \u0026#34;arn:aws:ecs:us-west-2:\u0026lt;AWS ID\u0026gt;:task-definition/notes-api-task:9\u0026#34;, \u0026#34;containerDefinitions\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;notes-api-container\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;\u0026lt;AWS ID\u0026gt;.dkr.ecr.us-west-2.amazonaws.com/notes-api\u0026#34;, \u0026#34;cpu\u0026#34;: 0, \u0026#34;portMappings\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;notes-api-container-8000-tcp\u0026#34;, \u0026#34;containerPort\u0026#34;: 8000, \u0026#34;hostPort\u0026#34;: 80, \u0026#34;protocol\u0026#34;: \u0026#34;tcp\u0026#34; } ], \u0026#34;essential\u0026#34;: true, \u0026#34;environment\u0026#34;: [], \u0026#34;mountPoints\u0026#34;: [ { \u0026#34;sourceVolume\u0026#34;: \u0026#34;keys\u0026#34;, \u0026#34;containerPath\u0026#34;: \u0026#34;/app/keys.env\u0026#34;, \u0026#34;readOnly\u0026#34;: false } ], \u0026#34;volumesFrom\u0026#34;: [] } ], \u0026#34;family\u0026#34;: \u0026#34;notes-api-task\u0026#34;, \u0026#34;revision\u0026#34;: 9, \u0026#34;volumes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;keys\u0026#34;, \u0026#34;host\u0026#34;: { \u0026#34;sourcePath\u0026#34;: \u0026#34;/home/ec2-user/keys.env\u0026#34; } } ], \u0026#34;status\u0026#34;: \u0026#34;ACTIVE\u0026#34;, \u0026#34;requiresAttributes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;com.amazonaws.ecs.capability.ecr-auth\u0026#34; } ], \u0026#34;placementConstraints\u0026#34;: [], \u0026#34;compatibilities\u0026#34;: [ \u0026#34;EC2\u0026#34; ], \u0026#34;requiresCompatibilities\u0026#34;: [ \u0026#34;EC2\u0026#34; ], \u0026#34;cpu\u0026#34;: \u0026#34;1024\u0026#34;, \u0026#34;memory\u0026#34;: \u0026#34;100\u0026#34;, \u0026#34;registeredAt\u0026#34;: \u0026#34;2023-11-15T01:19:16.910Z\u0026#34;, \u0026#34;registeredBy\u0026#34;: \u0026#34;arn:aws:iam::\u0026lt;AWS ID\u0026gt;:user/\u0026lt;AWS USER\u0026gt;\u0026#34;, \u0026#34;tags\u0026#34;: [] } One more commit later and I had 5/5 check marks!\nConclusion I successfully set up my first CI/CD pipeline with Github Actions. I can now work on the backend of my notes app and easily deploy new versions to production by simply pushing a new commit to the main branch.\nI really enjoyed this one. It was so gratifying to put in a lot of effort and to see the results of that effort in the form of check marks when each step succeeded. There were certainly times I entered a kind of flow state working on this where the rest of the world dissolved away. I look forward to learning more and putting in the kind of effort that can lead to such a lovely state.\nCheers!\nRick\n   Resources  Configuring OpenID Connect in Amazon Web Services (Github Docs) - https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/configuring-openid-connect-in-amazon-web-services Connecting GitHub Actions To AWS Using OIDC (StratusGrid) - https://www.youtube.com/watch?v=mel6N62WZb0 Deploying to Amazon Elastic Container Service (Github Docs) - https://docs.github.com/en/actions/deployment/deploying-to-your-cloud-provider/deploying-to-amazon-elastic-container-service  ","permalink":"https://rickt.io/posts/08-setting-up-github-actions-to-deploy-to-ecs/","summary":"Introduction I was excited to tackle today\u0026rsquo;s topic of using a CI/CD tool, GitHub Actions, to automate part of the deployment of my Notes app. One of my favorite aspects of working with technology is getting to automate things. Much of what I built in my last post was done manually in a graphical interface, which wasn\u0026rsquo;t as exciting for me, but was a necessary step in the learning process. So, let\u0026rsquo;s see how I automated part of the deployment process using GitHub Actions.","title":"Setting Up GitHub Actions to Deploy to ECS"},{"content":"Introduction In my latest efforts to learn more about DevOps tools and practices, I set out to deploy the Docker image for my FastAPI app API to AWS. You can read about the process of building the image in my last post.\nInitially, I was overwhelmed by the many ways to run containers on AWS. I mistakenly decided to try Lambda - partially to use the most \u0026ldquo;serverless\u0026rdquo; method I could, but mostly to be as cheap as possible. Many painful hours of not getting it to work later, and I decided to try out ECS instead. Either way, first I needed to push my Docker image to AWS ECR.\nECR Setup I was able to do this part using the AWS CLI. First I created a new ECR repository:\naws ecr create-repository --repository-name notes-api Next, AWS gave instructions on how to authenticate the local Docker utility to my AWS:\naws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin \u0026lt;AWS ID\u0026gt;.dkr.ecr.us-west-2.amazonaws.com I was able to get my AWS id using this command:\naws sts get-caller-identity Then, AWS required a specific taging convention:\ndocker tag notes-api:latest \u0026lt;AWS ID\u0026gt;.dkr.ecr.us-west-2.amazonaws.com/notes-api:latest Finally, I was able to push the image to ECR using:\ndocker push \u0026lt;AWS ID\u0026gt;.dkr.ecr.us-west-2.amazonaws.com/notes-api:latest I verified that my image showed up in the web console and copied the image URI, so it was time to spin up an ECS cluster.\nECS Setup I wasn\u0026rsquo;t finding any real resources for setting up ECS using the CLI, so I did this part using the web console. I hate having to rely on a graphical interface, so I plan on learning some form of IaC for this in the future.\nFirst, I went to the old web interface because it matched the guides I was using. Then I went to Create Cluster and selected EC2 Linux + Networking.\nI then changed the following fields:\n Cluster Name: notes-api EC2 instance type: t3.nano Key pair: an existing key pair - this isn\u0026rsquo;t necessary, but I wanted to have it so that I could connect to the ec2 instance if I needed to troubleshoot, which I ended up needing. VPC: I chose one I had setup previously Security group: I chose one I had setup previously Auto assign public IP: Enabled  After hitting create, I went to Task Definitions then Create new Task Definition and selected EC2.\nFor this section I entered the following:\n Task definition name: notes-api-task Task Memory: 100 Task CPU: 1024  And under Container definitions, I clicked Add container, gave it a name, pasted my container URI from the ECR page, and set a port mapping of 80 to 8000 to match the Uvicorn service that runs in the container. I left the rest as defaults and clicked Add.\nBack at the Create Cluster page, I left the rest as default and clicked Create.\nThen, I went back to the Clusters page, selected my \u0026ldquo;notes-api\u0026rdquo; cluster, selected the Tasks tab, and clicked Run new Task. On the next screen, I selected EC2 and clicked Run Task\nAt this point, I went to the EC2 console and found the public IP for my new EC2 instance. With that, I was able to verify that the root GET method, which was a simple string return, was working. The other routes, which used an external API call to Mongo Atlas, were not.\nI was able to get into the EC2 instance and read the output from the Docker container, which described a failed connection to the Mongo API. At first I thought it must be the Security Group firewall blocking it on AWS, so I first tried opening the port it needed (27017), and opening the whole thing when that didn\u0026rsquo;t work. Eventually I logged into my Atlas console and realized that there was an IP allow list on that end. Once I added the EC2 instance\u0026rsquo;s IP, everything worked! Of course I removed the allow all rule from the Security Group, leaving the 27017 port open.\nAdding the Front-End Now that the API was fully functioning, I just needed to put up my front-end. This was quite easy as I had done this before and wrote down how to host it on S3 using the CLI in a previous post.\nOnce the front-end was up and a few minutes after adding the DNS record, I had my app fully running at http://notes.rickt.io/\nConclusion This one took a lot of effort to get through. I had been struggling to focus and spend my time productively working on this ongoing project. One thing that has helped a lot is picking up a used ThinkPad and working outside of my home at coffee shops. Not only has it given me the ability to stay on task, but it has forced me to become even more familiar with Git when I switch from one workstation to the other.\nI\u0026rsquo;m also immensely grateful to my partner, Jen Hernandez for all her support in this endeavor.\nCheers!\nRick\n   Resources  Github repo for this app - https://github.com/rickthackeray/notes \\ AWS cloud demos' ECS guide - https://www.youtube.com/watch?v=PgyFrkJaNys \\  ","permalink":"https://rickt.io/posts/07-deploying-fastapi-to-aws-ecs/","summary":"Introduction In my latest efforts to learn more about DevOps tools and practices, I set out to deploy the Docker image for my FastAPI app API to AWS. You can read about the process of building the image in my last post.\nInitially, I was overwhelmed by the many ways to run containers on AWS. I mistakenly decided to try Lambda - partially to use the most \u0026ldquo;serverless\u0026rdquo; method I could, but mostly to be as cheap as possible.","title":"Deploying a FastAPI Container to AWS ECS"},{"content":"Introduction In my ongoing effort to learn the tools and practices for DevOps engineering, today I wanted to containerize the FastAPI backend of the notes app that I created previously, using Docker.\nGetting Started I started off by creating a Dockerfile. Well, I really started off by questioning whether my flat folder structure was best practice (it clearly isn\u0026rsquo;t), but decided that it would be better to focus my attention on the learning task at hand. So, I created a file called Dockerfile in the root of my backend folder and added the following:\nFROMpython:3.10-slimCOPY . ./appWORKDIR/appRUN pip3 install -r requirements.txtEXPOSE8000CMD [\u0026#34;uvicorn\u0026#34;, \u0026#34;main:app\u0026#34;, \u0026#34;--host=0.0.0.0\u0026#34;]This is the file Docker uses when building the image. Here\u0026rsquo;s a quick rundown of what it does:\nFROM python:3.10-slim - The image it starts with, which is made up a very trimmed down version of Linux and Python 3.10\nCOPY . ./app - Copies all files in the local folder to the image\u0026rsquo;s filesystem under /app\nWORKDIR /app - Sets the working directory to /app for commands that follow\nRUN pip3 install -r requirements.txt - Runs the command to install python dependencies with pip\nEXPOSE 8000 - This tells Docker that the container listens at port 8000 (but doesn\u0026rsquo;t open it to the host)\nCMD [\u0026quot;uvicorn\u0026quot;, \u0026quot;main:app\u0026quot;, \u0026quot;--host=0.0.0.0\u0026quot;] - Sets the default execution command for running the container, which can be overwritten at runtime\nWith the Dockerfile ready, I could build a new image by running the docker build command with an option name (tag): docker build -t notes-api .\nThat worked, and I was able to run the container, mapping port 8000 from the host to the container using: docker run -p 8000:8000 notes-api\nIt all worked, great!\nThere was a problem, though. I knew full well that my new image now contained my MongoDB Atlas credentials that would be exposed to the internet should I upload it to Docker Hub - not a best practice to say the least. So I needed to find a solution for mitigating that security risk.\nHandling Secrets It was surprising difficult to find information on how to handle secrets when creating images without greatly increasing complexity. It seems like everyone uses a secrets manager like Hashicorp\u0026rsquo;s Vault or doesn\u0026rsquo;t want to talk about their practice, perhaps because they aren\u0026rsquo;t proud of it. I would like to learn Vault, but that\u0026rsquo;s going to have to be further down the road. So for now here is what I came up with:\nI had already built my code to keep it\u0026rsquo;s secrets in a separate keys.env file, so I just needed to exclude that at the image build, then re-add it at container runtime. To do that, I created a .dockerignore with the following:\nenv/ __pycache__/ *.env *.env.* env.* docker-compose.yaml I reran the build, then ran the container using the same docker run -p 8000:8000 notes-api command and got this:\nWhich seemed bad, but then I remembered that it should break, because it no longer had the credentials! So I had to figure out how to inject the keys.env into the container at runtime, which led me to learning about Docker Volumes. Essentially, I could give the container access to part of the host\u0026rsquo;s filesystem. Here is the command I used for that: docker run -p 8000:8000 -v ./:/app notes-api\nSuccess!\nThere was just one more quality of life improvement I wanted to implement. I knew from working with Docker before (see Docker for Home Services) that defining all of your runtime arguments using the CLI was not ideal, especially as a project grows in complexity. So, the last step was to create a docker-compose file.\nAdding Docker-Compose I know this isn\u0026rsquo;t totally necessary with such a simple app that will run in only one container, but it\u0026rsquo;s relatively simple to implement and helps me build skills I anticipate needing in the future.\nAfter looking at my previous examples and some others specific to FastAPI, I ended up with this:\nname: notes services: api: build: . command: sh -c \u0026#34;uvicorn main:app --port=8000 --host=0.0.0.0\u0026#34; ports: - 8000:8000 volumes: - ./:/app Now I could run the container with a simple docker compose up\nCool!\nConclusion I enjoyed this step in learning the tools and practices for DevOps engineering. It\u0026rsquo;s a very rewarding feeling to build something and to tackle problems as they arise. I also appreciate having a project that I\u0026rsquo;ve built from scratch, which I can use to learn each new technology or practice.\nI am excited and looking forward to the next thing to learn. Until then\u0026hellip;\nCheers!\nRick\n","permalink":"https://rickt.io/posts/06-containerize-a-fastapi-app-with-docker/","summary":"Introduction In my ongoing effort to learn the tools and practices for DevOps engineering, today I wanted to containerize the FastAPI backend of the notes app that I created previously, using Docker.\nGetting Started I started off by creating a Dockerfile. Well, I really started off by questioning whether my flat folder structure was best practice (it clearly isn\u0026rsquo;t), but decided that it would be better to focus my attention on the learning task at hand.","title":"Containerize a FastAPI App with Docker"},{"content":"Introduction I recently had another use case for writing a script to parse some data. Normally, I would use Powershell for something like this (see Updating a CSV using Powershell), but this time I wanted to challenge myself to do it in Python. I\u0026rsquo;ve used Python a little, but I\u0026rsquo;m not as strong with it as I am with Powershell.\nThe problem I wanted to solve was that I needed to take a pool of digital cards for my MTG Cube, sort them into their color categories, shuffle the color-grouped cards, and deal them into \u0026ldquo;booster packs\u0026rdquo; - groups of 15 cards.\nInputs / Outputs The digital representation of my cards would come in as a CSV, which contained the name and color category information I would need.\nFor output, I needed to have a text file that listed one card per line, with a blank line to separate each \u0026ldquo;pack\u0026rdquo;, like so:\nBlade Splicer (nph) 4 Parallax Wave (vma) 37 Ninja of the Deep Hours (mb1) 446 Murktide Regent (mh2) 52 Headless Rider (vow) 118 Rotting Rats (con) 51 Dreadhorde Arcanist (war) 125 Pyrite Spellbomb (mrd) 232 Gaea\u0026#39;s Cradle (usg) 321 Lotus Cobra (ima) 174 Monastery Swiftspear (ktk) 118 Bonesplitter (mrd) 146 Prismatic Vista (mh1) 244 Goblin Rabblemaster (j22) 545 Reflector Mage (ogw) 157 Luminarch Aspirant (znr) 24 Anointed Peacekeeper (dmu) 2 Force of Will (2xm) 51 Remand (rav) 63 Liliana, the Last Hope (emn) 93 Ophiomancer (c13) 84 Cemetery Gatekeeper (vow) 148 Chandra, Hope\u0026#39;s Beacon (mom) 134 Jolrael, Mwonvuli Recluse (m21) 191 Vengevine (2xm) 185 Zagoth Triome (iko) 259 Overgrown Tomb (rav) 279 Chrome Mox (mrd) 152 Dack Fayden (cns) 42 Xander\u0026#39;s Lounge (snc) 260 Getting Started After some searching, I found that the third-party pandas module looked like the easiest way to handle the CSV. At first I tried doing all of the manipulation using their data frame data type, but eventually found it easier to just convert it all into a dictionary, with lists for each card.\nI used a dictionary so that I could sort the cards into separate groups by their color. Here\u0026rsquo;s what that looked like after struggling to get the right syntax:\nimport pandas input = \u0026#34;RickTesting.csv\u0026#34; cube = pandas.read_csv(input) colors = [\u0026#34;White\u0026#34;,\u0026#34;Blue\u0026#34;,\u0026#34;Black\u0026#34;,\u0026#34;Red\u0026#34;,\u0026#34;Green\u0026#34;] colors_abv = [\u0026#34;w\u0026#34;,\u0026#34;u\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;r\u0026#34;,\u0026#34;g\u0026#34;] cards = {} ## move from pandas data frame to dictionary of lists, organized by color for color in colors: cards[color] = cube[cube[\u0026#39;Color Category\u0026#39;].isin([color, colors_abv[colors.index(color)]])].values.tolist() cards[\u0026#34;Other\u0026#34;] = cube[~cube[\u0026#39;Color Category\u0026#39;].isin([*colors, *colors_abv])].values.tolist() The last line takes cards that don\u0026rsquo;t fall into the five colors and creates an \u0026ldquo;Other\u0026rdquo; group.\nI heavily used a python shell to troubleshoot each step along the way. For example, at this point I was able to confirm that the data looked like I expected it to by poking around the dictionary:\nThe next step of shuffling the cards in each color group was quite simple, after adding import random I added:\n# shuffle for color in cards: random.shuffle(cards[color]) The rest wasn\u0026rsquo;t so simple, and took the majority of a day.\nBuilding the \u0026ldquo;packs\u0026rdquo; I wanted to deal two cards of each color to each of 24 packs, then add any leftovers to the \u0026ldquo;Other\u0026rdquo; group of cards, and shuffle and deal that to the 24 packs. After a lot of effort, I ended up creating a new list for the \u0026ldquo;boosters\u0026rdquo; (packs), added the cards using three nested loops, and kept track of the last card so that I could add the remaining colored cards to the \u0026ldquo;Other\u0026rdquo; group. Here\u0026rsquo;s what all of that eventually looked like:\nnum_boosters = 24 num_cards = 15 num_each_color = 2 last = {} boosters = [] # add colored cards to boosters for booster in range(0, num_boosters): boosters.append([]) for color in colors: for i in range(0, num_each_color): position = i + booster * num_each_color if (position \u0026lt; len(cards[color])): boosters[booster].append(cards[color][position]) last[color] = position # add rest of colored cards to other for color in colors: for i in range(last[color]+1, len(cards[color])): cards[\u0026#34;Other\u0026#34;].append(cards[color][i]) random.shuffle(cards[\u0026#34;Other\u0026#34;]) # add the rest to boosters position = 0 for booster in range(0, len(boosters)): for i in range(0, num_cards - len(boosters[booster])): boosters[booster].append(cards[\u0026#34;Other\u0026#34;][position]) position += 1 This is usually when I think \u0026ldquo;there must be a more elegant way to do this\u0026rdquo;, but try not to get lost in the rabbit hole of searching for that. Learning comes incrementally - it\u0026rsquo;s important to take small bites.\nPutting it All Together Now that I had my boosters, I just needed to output them. I was initially going to write them to a file in the script, but decided that outputting using print() might make more sense. This seemed the same as the typically preferred method in Powershell (write-output). The idea being that it gives greater control at runtime. Either way, here\u0026rsquo;s what that looked like:\nfor booster in boosters: for card in booster: print(f\u0026#34;{card[0]}({card[4]}) {card[5]}\u0026#34;) print(\u0026#34;\u0026#34;) Then I was able to create a new text file of boosters by running\npython3 make-boosters.py \u0026gt; boosters.txt Conclusion It was a joy, if a struggle, to dig into a language I\u0026rsquo;ve been wanting to learn more of. I feel grateful that I had a problem to solve where Python was a good solution. I look forward to using it more in the future.\nYou can find the entire script on my Github Gists.\nCheers!\nRick\n","permalink":"https://rickt.io/posts/05-parsing-csv-data-using-python/","summary":"Introduction I recently had another use case for writing a script to parse some data. Normally, I would use Powershell for something like this (see Updating a CSV using Powershell), but this time I wanted to challenge myself to do it in Python. I\u0026rsquo;ve used Python a little, but I\u0026rsquo;m not as strong with it as I am with Powershell.\nThe problem I wanted to solve was that I needed to take a pool of digital cards for my MTG Cube, sort them into their color categories, shuffle the color-grouped cards, and deal them into \u0026ldquo;booster packs\u0026rdquo; - groups of 15 cards.","title":"Parsing CSV Data Using Python"},{"content":"Introduction I\u0026rsquo;ve been wanting to learn more about CI/CD tools and techniques. To do this I could use an existing project, but I also want to get more reps in with the development process, so I decided to start a new project. A note taking app sounded interesting and simple enough to cover what I want to learn. So, I started by setting up a basic structure using the FARM stack.\nThe FARM stack stands for:\nFastAPI - a Python framework for easily writing a back end API.\nReact - a Javascript library used for writing interactive front end interfaces.\nMongoDB - a NoSQL document DB with low latency and high scalability.\nI chose this stack because I have a little experience with FastAPI and React and wanted to check out MongoDB, as I had only used SQLite before and wanted to see how a NoSQL document DB works.\nPrerequisites Before starting, I already had the following installed:\\\n NodeJS Git Python  pipenv: install by running pip install pipenv --user    I used Linux (Ubuntu) for my workstation, but this process works for Windows and MacOS as well.\nFastAPI First, I created a project folder along with two subfolders for the front end and back end, as these will be containerized and deployed separately at a later date.\n~/coding/projects/notes/frontend\n~/coding/projects/notes/backend\nThen, I created a backend/requirements.txt with the following:\nfastapi == 0.85.1 uvicorn == 0.18.3 motor == 3.2.0 Uvicorn is an ASGI python web server commonly used with FastAPI. Motor is an asynchronous driver for interacting with MongoDB.\nNext, I started a python virtual environment with pipenv shell and installed the dependencies with pipenv install -r requirements.txt\nThen I created a file backend/main.py with the following:\nfrom fastapi import FastAPI from fastapi.middleware.cors import CORSMiddleware app = FastAPI() origins = [\u0026#39;https://localhost:3000\u0026#39;] app.add_middleware( CORSMiddleware, allow_origins = origins, allow_credentials = True, allow_methods=[\u0026#34;*\u0026#34;], allow_headers=[\u0026#34;*\u0026#34;] ) @app.get(\u0026#34;/\u0026#34;) def read_root(): return {\u0026#34;Hello\u0026#34;:\u0026#34;World\u0026#34;} To make sure this worked, I started the Uvicorn server locally with uvicorn main:app --reload and got the \u0026ldquo;Hello: World\u0026rdquo; response at http://127.0.0.1:8000/. You can also get an automatically generated interactive list of all your API methods by appending /docs to the url. This is one of my favorite features of FastAPI.\nMongoDB You can setup a MongoDB server locally, but I wanted to get more experience working with SASS, so I signed up to use MongoDB\u0026rsquo;s Atlas free tier. I also installed MongoDB Shell and MongoDB Compass - the command line and graphical interfaces.\nIn the Atlas web interface, I then created a cluster, choosing AWS, and got the connection information by clicking the connect button in the main dashboard and following the direction for Mongo Shell. Once connected, I created a new database called \u0026ldquo;notes\u0026rdquo; by running use notes\nI wanted to add some dummy data, so, to create a collection called board01 and insert some data I ran:\ndb.board01.insertOne({title:\u0026quot;hello\u0026quot;,description:\u0026quot;there\u0026quot;})  To confirm, I viewed the results in Compass: Connecting FastAPI to MongoDB To connect the FastAPI backend to the Mongo database, I created a file, database.py. Here, I used the motor python module to create functions that will be called by main.py. I also created a separate keys.env to more securly insert my Mongo Atlas credentials, then loaded those using dotenv. Here\u0026rsquo;s what that ended up looking like:\nimport os from bson import ObjectId import pydantic import motor.motor_asyncio from dotenv import load_dotenv pydantic.json.ENCODERS_BY_TYPE[ObjectId]=str load_dotenv(\u0026#34;keys.env\u0026#34;) mongo_user = os.getenv(\u0026#34;MONGO_USER\u0026#34;) mongo_pass = os.getenv(\u0026#34;MONGO_PASS\u0026#34;) uri = \u0026#39;mongodb+srv://\u0026#39; + mongo_user + \u0026#39;:\u0026#39; + mongo_pass + \u0026#39;@cluster0.4ybbrum.mongodb.net/?retryWrites=true\u0026amp;w=majority\u0026#39; client = motor.motor_asyncio.AsyncIOMotorClient(uri) database = client.notes collection = database.board01 async def get_all_cards(): cards = [] cursor = collection.find({}) async for document in cursor: cards.append(document) return cards async def create_card(card): result = await collection.insert_one(card) return card async def update_card(id, card): document = card await collection.replace_one({\u0026#34;_id\u0026#34;: ObjectId(id)}, document) return document async def remove_card(id): await collection.delete_one({\u0026#34;_id\u0026#34;: ObjectId(id)}) return True The keys.env file looked something like:\nMONGO_USER=myuser MONGO_PASS=mypassphrase With that in place, I added the following functions to main.py:\n@app.get(\u0026#34;/cards\u0026#34;) async def get_cards(): response = await database.get_all_cards() return response @app.post(\u0026#34;/card\u0026#34;) async def add_card(title: str, description: str): card = {\u0026#39;title\u0026#39;: title, \u0026#39;description\u0026#39;: description} response = await database.create_card(card) return response @app.put(\u0026#34;/card\u0026#34;) async def update_card(id: str, title: str, description: str): card = {\u0026#39;title\u0026#39;: title, \u0026#39;description\u0026#39;: description} response = await database.update_card(id, card) return response @app.delete(\u0026#34;/card\u0026#34;) async def delete_card(id): response = await database.remove_card(id) return response I used the interface that is generated by FastAPI at http://127.0.0.1:8000/docs to confirm that each function worked. With that, the basic backend was finished and I just needed to create the frontend interface.\nReact In the /notes/frontend folder, I initialized a new React project by running:\nnpx create-react-app\nThen, I ran the local dev server using:\nnpm start\nAfter clearing out the default code from App.js, I created an initial load function, a delete function , and an add function. These all form API calls using the vanilla fetch function and modify the working data using React\u0026rsquo;s useState hook.\nimport React, {useState, useEffect} from \u0026#34;react\u0026#34; import \u0026#39;./App.css\u0026#39;; import Card from \u0026#34;./Card\u0026#34; import AddCard from \u0026#34;./AddCard\u0026#34; function App() { const [cardData, setCardData] = useState([]) const host = \u0026#39;http://10.1.1.121:8000\u0026#39; useEffect(() =\u0026gt; { loadCardData() },[]) function loadCardData() { fetch(host + \u0026#39;/cards\u0026#39;) .then(response =\u0026gt; response.json()) .then(data =\u0026gt; { data = data.map(prev =\u0026gt; { return {...prev} }) setCardData(data) }) } function deleteCard(id) { fetch(host + \u0026#39;/card?id=\u0026#39; + id, {method: \u0026#39;DELETE\u0026#39;}) .then(setCardData(prev =\u0026gt; prev.filter(card =\u0026gt; card._id !== id))) } function addCard(props) { fetch(host + \u0026#39;/card?title=\u0026#39; + props.title + \u0026#39;\u0026amp;description=\u0026#39; + props.description, {method: \u0026#39;POST\u0026#39;}) .then(response =\u0026gt; response.json()) .then(data =\u0026gt; { console.log(data) setCardData(prev =\u0026gt; [...prev, data]) }) } const cards = cardData.map(card =\u0026gt; { return \u0026lt;Card key = {card._id} title = {card.title} description = {card.description} delete = {() =\u0026gt; deleteCard(card._id)} /\u0026gt; }) return ( \u0026lt;main className=\u0026#34;main-container\u0026#34;\u0026gt; \u0026lt;AddCard addCard={addCard}/\u0026gt; \u0026lt;div className=\u0026#34;cards\u0026#34;\u0026gt;{cards}\u0026lt;/div\u0026gt; \u0026lt;/main\u0026gt; ) } export default App; In addition to the API interfacing functions, I created two React components. The first was AddCard.js, which creates an input form used for adding a new notecard.\nimport React, {useState} from \u0026#34;react\u0026#34; import \u0026#34;./AddCard.css\u0026#34; export default function AddCardForm(props) { const [formData, setFormData] = useState({ title: \u0026#34;\u0026#34;, description: \u0026#34;\u0026#34; }) function handleSubmit (event) { event.preventDefault() props.addCard(formData) setFormData({ title: \u0026#34;\u0026#34;, description: \u0026#34;\u0026#34; }) } function handleChange(event) { setFormData(prev =\u0026gt; { return { ...prev, [event.target.name]: event.target.value } }) } return ( \u0026lt;div className=\u0026#34;addcard\u0026#34;\u0026gt; \u0026lt;form onSubmit={handleSubmit}\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; placeholder=\u0026#34;Title\u0026#34; onChange={handleChange} className=\u0026#34;title\u0026#34; name=\u0026#34;title\u0026#34; value={formData.title} /\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; placeholder=\u0026#34;Description\u0026#34; onChange={handleChange} className=\u0026#34;description\u0026#34; name=\u0026#34;description\u0026#34; value={formData.description} /\u0026gt; \u0026lt;button\u0026gt; \u0026lt;svg className=\u0026#34;submit\u0026#34; viewBox=\u0026#34;0 0 24 24\u0026#34; role=\u0026#34;img\u0026#34; xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; aria-labelledby=\u0026#34;returnIconTitle\u0026#34; fill=\u0026#34;none\u0026#34; color=\u0026#34;#000000\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Enter\u0026lt;/title\u0026gt; \u0026lt;path d=\u0026#34;M19,8 L19,11 C19,12.1045695 18.1045695,13 17,13 L6,13\u0026#34;/\u0026gt; \u0026lt;polyline points=\u0026#34;8 16 5 13 8 10\u0026#34;/\u0026gt;\u0026lt;/svg\u0026gt; \u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/div\u0026gt; ) } The second component was for the cards themselves, Card.js:\nimport React from \u0026#34;react\u0026#34; import \u0026#34;./Card.css\u0026#34; export default function Card(props) { return ( \u0026lt;div className=\u0026#34;card\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;title\u0026#34;\u0026gt;{props.title}\u0026lt;/div\u0026gt; \u0026lt;div className=\u0026#34;text\u0026#34;\u0026gt;{props.description}\u0026lt;/div\u0026gt; \u0026lt;div className=\u0026#34;footer\u0026#34;\u0026gt; \u0026lt;button onClick={props.delete}\u0026gt; \u0026lt;svg className=\u0026#34;delete\u0026#34; viewBox=\u0026#34;0 0 1024 1024\u0026#34; version=\u0026#34;1.1\u0026#34; xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Delete\u0026lt;/title\u0026gt; \u0026lt;path d=\u0026#34;M512 897.6c-108 0-209.6-42.4-285.6-118.4-76-76-118.4-177.6-118.4-285.6 0-108 42.4-209.6 118.4-285.6 76-76 177.6-118.4 285.6-118.4 108 0 209.6 42.4 285.6 118.4 157.6 157.6 157.6 413.6 0 571.2-76 76-177.6 118.4-285.6 118.4z m0-760c-95.2 0-184.8 36.8-252 104-67.2 67.2-104 156.8-104 252s36.8 184.8 104 252c67.2 67.2 156.8 104 252 104 95.2 0 184.8-36.8 252-104 139.2-139.2 139.2-364.8 0-504-67.2-67.2-156.8-104-252-104z\u0026#34; fill=\u0026#34;\u0026#34; /\u0026gt; \u0026lt;path d=\u0026#34;M707.872 329.392L348.096 689.16l-31.68-31.68 359.776-359.768z\u0026#34; fill=\u0026#34;\u0026#34; /\u0026gt; \u0026lt;path d=\u0026#34;M328 340.8l32-31.2 348 348-32 32z\u0026#34; fill=\u0026#34;\u0026#34; /\u0026gt; \u0026lt;/svg\u0026gt; \u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; ) } I\u0026rsquo;m glossing over a lot of the difficulty I had in creating the React code. If you\u0026rsquo;re looking for a good resource for learning React, I recommend checking out Scrimba\u0026rsquo;s course by Bob Ziroll. It is one of the best resources for any technology that I\u0026rsquo;ve come across.\nAfter that I spent way too long working on css to create a passable aesthetic, and I\u0026rsquo;m pretty happy how it turned out:\nConclusion I am looking forward to using this project to learn more about Docker, CI/CD, IAC, and other devops tools.\nYou can find the repo with all the code here - https://github.com/rickthackeray/notes\nCheers!\nRick\n","permalink":"https://rickt.io/posts/04-creating-a-basic-notes-app-using-the-farm-stack/","summary":"Introduction I\u0026rsquo;ve been wanting to learn more about CI/CD tools and techniques. To do this I could use an existing project, but I also want to get more reps in with the development process, so I decided to start a new project. A note taking app sounded interesting and simple enough to cover what I want to learn. So, I started by setting up a basic structure using the FARM stack.","title":"Creating a Notes App with FARM Stack"},{"content":"Introduction I love Powershell. Over my IT career I have spent a lot of time learning how to use it to automate all kinds of things: batch creation of Active Directory accounts, interfacing with APIs, tracking network congestion, and conducting mass software deployments. During the past year I\u0026rsquo;ve been focused on learning DevOps practices, where Linux is king and Powershell is rarely mentioned - and I\u0026rsquo;ve missed using it.\nFortunately, I had a use case recently where Powershell would be a great tool. I needed to update a CSV file. Specifically, I needed to add \u0026ldquo;tag\u0026rdquo; attributes to cards for my MTG Cube. This can be done in the web app, but it\u0026rsquo;s time consuming. There is also no API, but you can export a CSV, then modify and upload it to update your cube. So I wrote a tool to do just that.\nSetup I often start writing a script by interactively working out what needs to happen in a shell. So, After downloading the CSV, I imported it to a variable to see what the format was like\nLooked good, but I needed a card that had some tags on it in order to match their format, so I ran the command:\n$csv | where {$_.tags -ne \u0026quot;\u0026quot;}\nand found one that had multiple tags:\nNow that I knew the format the web app expected, I tried editing a card (row) and saving it to the same CSV file:\nThen I uploaded it the web app and saw that the tags updated as expected.\nAdding Data in Bulk I had the basics laid out, now I needed to scale it up. The way I would do that would be to use a different web app that essentially let me select cards and export them to a text file. So, I would do that for one tag at a time, using the tag as the filename. Example:\nSpells.txt 1 Bazaar of Baghdad 1 Kolaghan's Command 1 Grapple with the Past 1 Bonecrusher Giant 1 Collective Brutality 1 Territorial Kavu 1 Scourge of Nel Toth 1 Oona's Prowler 1 Divest I had to parse one of these text files, striping the number, and add the name of the file (the tag) to each CSV entry that matched a line in the text file. Along with that I realized I needed to cover the common cases of a card already having the tag. Here\u0026rsquo;s what I eventually ended up with:\n$cards = Get-Content $DeckFile foreach ($card in $cards) { $card = $card.Substring(2) $csv | ForEach-Object { if ($_.name -eq $card) { if (!($_.tags.Contains($tag))) { if ($_.tags -eq \u0026#34;\u0026#34;) { $_.tags = $tag } else { $_.tags = $_.tags + \u0026#34;;\u0026#34; + $tag } Write-Output \u0026#34;Adding $($tag)to $($card)\u0026#34; } else { Write-Verbose \u0026#34;$($card)is already tagged with $($tag)\u0026#34; } } } } It\u0026rsquo;s not the prettiest of logic, but it worked. The only improvement I wanted to make was to generalize it a bit more into a cmdlet.\nPutting it All Together Creating a cmdlet is pretty easy, but I do it seldom enough that I always forget the details. Luckily I made a gist for it on GitHub. So, a copy/paste and few modifications later and I had the final product:\nfunction Add-TagsToCards { [CmdletBinding()] Param( [Parameter( Position=0, Mandatory=$True, ValueFromPipeline=$True )] [string] $DeckFile, [Parameter( Position=1, Mandatory=$False )] [string] $CubeCSV = \u0026#34;./RicksAwesomeCube.csv\u0026#34;, [string] $OutputCSV = \u0026#34;./RicksAwesomeCube.csv\u0026#34; ) Process { $file = Get-ChildItem $DeckFile $tag = $file.BaseName $csv = Import-Csv $CubeCSV $cards = Get-Content $DeckFile foreach ($card in $cards) { $card = $card.Substring(2) $csv | ForEach-Object { if ($_.name -eq $card) { if (!($_.tags.Contains($tag))) { if ($_.tags -eq \u0026#34;\u0026#34;) { $_.tags = $tag } else { $_.tags = $_.tags + \u0026#34;;\u0026#34; + $tag } Write-Output \u0026#34;Adding $($tag)to $($card)\u0026#34; } else { Write-Verbose \u0026#34;$($card)is already tagged with $($tag)\u0026#34; } } } } $csv | Export-Csv $OutputCSV } } Conclusion Now when I make a new set of selections, I can export the cube (csv) and deck (txt) files and run the command\nAdd-TagsToCards Spells.txt RicksAwesomeCube.csv\nand re-import the modified csv. This is an improvement over the process of tagging cards in the web interface. Of course it would be even better if there was an API, but this works.\nIt was nice to be able to dust off some old skills and work with Powershell again. I hope to be able to use it in the future as I work towards learning DevOps engineering skills.\nUntil next time,\nCheers!\nRick\n","permalink":"https://rickt.io/posts/03-updating-a-csv-using-powershell/","summary":"Introduction I love Powershell. Over my IT career I have spent a lot of time learning how to use it to automate all kinds of things: batch creation of Active Directory accounts, interfacing with APIs, tracking network congestion, and conducting mass software deployments. During the past year I\u0026rsquo;ve been focused on learning DevOps practices, where Linux is king and Powershell is rarely mentioned - and I\u0026rsquo;ve missed using it.\nFortunately, I had a use case recently where Powershell would be a great tool.","title":"Updating a CSV Using Powershell"},{"content":"Introduction A while back I built a basic CRUD app to learn more about app development. For a while I hosted it on an EC2 instance, but shut it down at some point. To get it back up an running I decided to split the front end hosting off and use S3.\nI had set up a simple website on S3 using the graphical interface before, so this time I wanted to challenge myself by using the command line with AWS CLI.\nSet Up AWS CLI I\u0026rsquo;m currently running Linux as my daily driver, so installation was as easy as running:\nsudo apt install awscli From there I needed to create an IAM access key and add it to the configuration. To do that, I went:\n IAM -\u0026gt; Users -\u0026gt; \u0026lt;my user\u0026gt; Security credentials tab -\u0026gt; create access key button Command Line Interface radio button -\u0026gt; Next tag is optional - I put \u0026ldquo;cli\u0026rdquo; Save the access and secret keys - I downloaded them to a .csv.  After that, I ran aws configure, entering the public \u0026ldquo;access\u0026rdquo; key and the private \u0026ldquo;secret\u0026rdquo; key. At first I thought it wanted filenames which store each key, but it just wanted the keys directly\nTo confirm it was working, I ran aws s3 ls and it was!\nCreate and Configure S3 Bucket Here are the steps and commands I used next to create and configure the S3 bucket:\n1. Create the bucket:\naws s3api create-bucket --bucket films.rickt.io --region us-west-2 --create-bucket-configuration LocationConstraint=us-west-2 I confirmed using aws s3 ls\n2. Enable static website hosting:\naws s3 website s3://films.rickt.io --index-document index.html --error-document error.html 3. Remove public access block\naws s3api delete-public-access-block --bucket films.rickt.io 4. Add policy to bucket to allow public read access\nTo do this I created a file with the following:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::films.rickt.io/*\u0026#34; } ] } which I saved as bucket-policy.json\nThen, from the same folder as the file I ran the command:\naws s3api put-bucket-policy --bucket films.rickt.io --policy file://bucket-policy.json To confirm everything worked, I went to the new bucket in S3 and selected Permissions, from there I could see the settings were all in place\nUpload content Everything looked good and was ready for files, so I navigated to the /build folder of my front-end (built using React) and ran the following:\naws s3 cp ./ s3://films.rickt.io --recursive Everything was on the web interface, great!\nGet Website URL Last step - I went Properties tab of the bucket\u0026rsquo;s page and scrolled to the bottom, there was the url under Static website hosting\nAn update to my domain\u0026rsquo;s DNS records, some minutes later, and I had the webpage up!\nConclusion This was a great project to get some experience using the AWS CLI. I was surprised how fairly straight-forward it was. It will be interesting to find out how useful it is going forward. I plan on learning Terraform soon and I\u0026rsquo;m curious how much that will replace the need for this tool.\nCheers!\nRick\n","permalink":"https://rickt.io/posts/02-deploying-a-website-to-s3-using-aws-cli/","summary":"Introduction A while back I built a basic CRUD app to learn more about app development. For a while I hosted it on an EC2 instance, but shut it down at some point. To get it back up an running I decided to split the front end hosting off and use S3.\nI had set up a simple website on S3 using the graphical interface before, so this time I wanted to challenge myself by using the command line with AWS CLI.","title":"Deploying a Website to S3 Using AWS CLI"},{"content":"Introduction Up to this point, I hadn\u0026rsquo;t used Docker very much. The organizations I worked at were traditional Windows environments with no in-house development. I could see the value in containers generally, but it was hard to justify and especially to convince others it was worth using in those environments.\nI\u0026rsquo;ve long hosted various services for myself at home. In the past these were ran on separate Windows VMs so that I could better learn the skills needed for the work I was doing. Looking to make the switch from traditional Systems Administration to DevOps Engineering, it was time to get comfortable with Docker. So, I decided to start running all my home services with it.\nToday I\u0026rsquo;m going over what I did to get the following services running, using Docker:\n Unifi Controller - network management for Ubiquiti gear PiHole - ad blocker DNS filter WireGuard - VPN  This was all done on a bare metal install of Ubuntu Server.\nInstalling Docker I used the official guide to install Docker, following the method to install from their apt repository:\nhttps://docs.docker.com/engine/install/ubuntu/#install-using-the-repository\nTo validate that it\u0026rsquo;s working, I ran:\nsudo docker run hello-world  I also added my user to the docker group, in order to remove the need to preface docker commands with sudo:\nsudo usermod -aG docker $USER newgrp docker  Configuring Docker Compose Files For each service, I created a folder in my home directory and added a docker-compose.yml file, based on the image creator\u0026rsquo;s example.\nUnifi Controller - /home/rick/unifi/docker-compose.yml:\n--- version: \u0026#34;2\u0026#34; services: unifi-controller: container_name: unifi-controller image: linuxserver/unifi-controller:latest environment: - PUID=1000 # for UserID - PGID=1000 # for GroupID - MEM_LIMIT=1024 # Optionally change the Java memory limit (-Xmx) (default is 1024M). ports: - 10001:10001/udp # Required for AP discovery - 8880:8880/tcp # Unifi guest portal HTTP redirect port - 8843:8843/tcp # Unifi guest portal HTTPS redirect port - 8443:8443/tcp # Unifi web admin port - 8080:8080/tcp # Required for device communication - 6789:6789/tcp # For mobile throughput test - 5514:5514/tcp # Remote syslog port - 3478:3478/udp # Unifi STUN port - 1900:1900/udp # Required for Make controller discoverable on L2 network option volumes: - ~/unifi/config:/config # Map a local directory to the container for configuration persistance restart: unless-stopped PiHole - /home/rick/pihole/docker-compose.yml:\n--- version: \u0026#34;3\u0026#34; services: pihole: container_name: pihole image: pihole/pihole:latest ports: - 53:53/tcp - 53:53/udp - 67:67/udp - 8888:80/tcp - 443:443/tcp environment: TZ: \u0026#39;US/Pacific\u0026#39; #this is the time zone WEBPASSWORD: \u0026#39;password\u0026#39; volumes: - \u0026#39;./etc-pihole/:/etc/pihole/\u0026#39; - \u0026#39;./etc-dnsmasq.d/:/etc/dnsmasq.d/\u0026#39; dns: - 127.0.0.1 - 1.1.1.1 restart: unless-stopped WireGuard - /home/rick/wireguard/docker-compose.yml:\n--- networks: wireguard: name: wireguard services: wireguard: container_name: wireguard image: lscr.io/linuxserver/wireguard:latest ports: - 51820:51820/udp environment: - PUID=1000 - PGID=1000 - SERVERURL=auto - SERVERPORT=51820 - PEERS=rickphone - PEERDNS=208.67.222.222 - TZ=US/Pacific networks: - wireguard volumes: - /lib/modules:/lib/modules - ~/wireguard:/config restart: unless-stopped cap_add: - NET_ADMIN - SYS_MODULE sysctls: - net.ipv4.conf.all.src_valid_mark=1 Running each Docker Compose file I didn\u0026rsquo;t want to have to manually start each container any time the server rebooted, so I created the following script:\n#!/bin/bash while read service; do echo \u0026#34;Starting $service...\u0026#34; docker compose -f ./$service/docker-compose.yml up -d done \u0026lt; services.txt with a services.txt file, to easily remove a service if needed later:\nunifi pihole wireguard and added the line to crontab using crontab -e, so that it executes at startup:\n@reboot sh /home/rick/start-services.sh Conclusion Overall, this was a fairly pleasant learning experience. I left out some of the troubles I encountered and details with setting up some of the specifics of each server, but I hope that this helps anyone looking to get their feet wet with Docker.\nCheers!\nRick\n   References and Resources https://learn.cantrill.io/courses/docker-fundamentals\nhttps://github.com/linuxserver/docker-unifi-controller\nhttps://github.com/linuxserver/docker-wireguard\nhttps://github.com/pi-hole/docker-pi-hole\n","permalink":"https://rickt.io/posts/01-docker-for-home-services/","summary":"Introduction Up to this point, I hadn\u0026rsquo;t used Docker very much. The organizations I worked at were traditional Windows environments with no in-house development. I could see the value in containers generally, but it was hard to justify and especially to convince others it was worth using in those environments.\nI\u0026rsquo;ve long hosted various services for myself at home. In the past these were ran on separate Windows VMs so that I could better learn the skills needed for the work I was doing.","title":"Docker for Home Services"}]