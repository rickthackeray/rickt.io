[{"content":"Introduction A while back I built a basic CRUD app to learn more about app development. For a while I hosted it on an EC2 instance, but shut it down at some point. To get it back up an running I decided to split the front end hosting off and use S3.\nI had setup a simple website on S3 using the graphical interface before, so this time I wanted to challenge myself by using the command line with AWS CLI.\nSetup AWS CLI I\u0026rsquo;m currently running Linux as my daily driver, so installation was as easy as running:\nsudo apt install awscli\nFrom there I needed to create an IAM access key and add it to the configuration. To do that, I went to IAM and Users, then under my user went to the Security credentials tab and clicked Create access key. I then selected Here are the steps I used to create a key:\n IAM -\u0026gt; Users -\u0026gt; \u0026lt;my user\u0026gt; Security credentials tab -\u0026gt; create access key button Command Line Interface radio button -\u0026gt; Next tag is optional - I put \u0026ldquo;cli\u0026rdquo; Save the access and secret keys - I downloaded them to a .csv.  After that, I ran aws configure, entering the public \u0026ldquo;access\u0026rdquo; key and the private \u0026ldquo;secret\u0026rdquo; key. At first I thought it wanted filenames which store each key, but it just wanted the keys directly\nTo confirm it was working, I ran aws s3 ls and it was!\nCreate and Configure S3 Bucket Here are the steps and commands I used next to create and configure the S3 bucket:\n1. Create the bucket:\naws s3api create-bucket --bucket films.rickt.io --region us-west-2 --create-bucket-configuration LocationConstraint=us-west-2\nI confirmed using aws s3 ls\n2. Enable static website hosting:\naws s3 website s3://films.rickt.io --index-document index.html --error-document error.html\n3. Remove public access block\naws s3api delete-public-access-block --bucket films.rickt.io\n4. Add policy to bucket to allow public read access\nTo do this I created a file with the following:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::films.rickt.io/*\u0026#34; } ] } which I saved as bucket-policy.json\nThen, from the same folder as the file I ran the command:\naws s3api put-bucket-policy --bucket films.rickt.io --policy file://bucket-policy.json\nTo confirm everything worked, I went to the new bucket in S3 and selected Permissions, from there I could see the settings were all in place\nUpload content Everything looked good and was ready for files, so I navigated to the /build folder of my front-end (built using React) and ran the following:\naws s3 cp ./ s3://films.rickt.io --recursive\nEverything was on the web interface, great!\nGet website URL Last step - I went Properties tab of the bucket\u0026rsquo;s page and scrolled to the bottom, there was the url under Static website hosting\nAn update to my domain\u0026rsquo;s DNS records, some minutes later, and I had the webpage up!\nConclusion This was a great project to get some experience using the AWS CLI. I was surprised how fairly straight-forward it was. It will be interesting to find out how useful it is going forward. I plan on learning Terraform soon and I\u0026rsquo;m curious how much that will replace the need for this tool.\nCheers!\nRick\n","permalink":"https://rickt.io/posts/deploying-a-website-to-s3-using-aws-cli/","summary":"Introduction A while back I built a basic CRUD app to learn more about app development. For a while I hosted it on an EC2 instance, but shut it down at some point. To get it back up an running I decided to split the front end hosting off and use S3.\nI had setup a simple website on S3 using the graphical interface before, so this time I wanted to challenge myself by using the command line with AWS CLI.","title":"Deploying a website to S3 using AWS CLI"},{"content":"Introduction Up to this point, I hadn\u0026rsquo;t used Docker very much. The organizations I worked at were traditional Windows environments with no in-house development. I could see the value in containers generally, but it was hard to justify and especially to convince others it was worth using in those environments.\nI\u0026rsquo;ve long hosted various services for myself at home. In the past these were ran on separate Windows VMs so that I could better learn the skills needed for the work I was doing. Looking to make the switch from traditional Systems Administration to DevOps Engineering, it was time to get comfortable with Docker. So, I decided to start running all my home services with it.\nToday I\u0026rsquo;m going over what I did to get the following services running, using Docker:\n Unifi Controller - network management for Ubiquiti gear PiHole - ad blocker DNS filter WireGuard - VPN  This was all done on a bare metal install of Ubuntu Server.\nInstalling Docker I used the official guide to install Docker, following the method to install from their apt repository:\nhttps://docs.docker.com/engine/install/ubuntu/#install-using-the-repository\nTo validate that it\u0026rsquo;s working, I ran:\nsudo docker run hello-world  I also added my user to the docker group, in order to remove the need to preface docker commands with sudo:\nsudo usermod -aG docker $USER newgrp docker  Configuring Docker Compose Files For each service, I created a folder in my home directory and added a docker-compose.yml file, based on the image creator\u0026rsquo;s example.\nUnifi Controller - /home/rick/unifi/docker-compose.yml:\n--- version: \u0026#34;2\u0026#34; services: unifi-controller: container_name: unifi-controller image: linuxserver/unifi-controller:latest environment: - PUID=1000 # for UserID - PGID=1000 # for GroupID - MEM_LIMIT=1024 # Optionally change the Java memory limit (-Xmx) (default is 1024M). ports: - 10001:10001/udp # Required for AP discovery - 8880:8880/tcp # Unifi guest portal HTTP redirect port - 8843:8843/tcp # Unifi guest portal HTTPS redirect port - 8443:8443/tcp # Unifi web admin port - 8080:8080/tcp # Required for device communication - 6789:6789/tcp # For mobile throughput test - 5514:5514/tcp # Remote syslog port - 3478:3478/udp # Unifi STUN port - 1900:1900/udp # Required for Make controller discoverable on L2 network option volumes: - ~/unifi/config:/config # Map a local directory to the container for configuration persistance restart: unless-stopped PiHole - /home/rick/pihole/docker-compose.yml:\n--- version: \u0026#34;3\u0026#34; services: pihole: container_name: pihole image: pihole/pihole:latest ports: - 53:53/tcp - 53:53/udp - 67:67/udp - 8888:80/tcp - 443:443/tcp environment: TZ: \u0026#39;US/Pacific\u0026#39; #this is the time zone WEBPASSWORD: \u0026#39;password\u0026#39; volumes: - \u0026#39;./etc-pihole/:/etc/pihole/\u0026#39; - \u0026#39;./etc-dnsmasq.d/:/etc/dnsmasq.d/\u0026#39; dns: - 127.0.0.1 - 1.1.1.1 restart: unless-stopped WireGuard - /home/rick/wireguard/docker-compose.yml:\n--- networks: wireguard: name: wireguard services: wireguard: container_name: wireguard image: lscr.io/linuxserver/wireguard:latest ports: - 51820:51820/udp environment: - PUID=1000 - PGID=1000 - SERVERURL=auto - SERVERPORT=51820 - PEERS=rickphone - PEERDNS=208.67.222.222 - TZ=US/Pacific networks: - wireguard volumes: - /lib/modules:/lib/modules - ~/wireguard:/config restart: unless-stopped cap_add: - NET_ADMIN - SYS_MODULE sysctls: - net.ipv4.conf.all.src_valid_mark=1 Running each Docker Compose file I didn\u0026rsquo;t want to have to manually start each container any time the server reboot, so I created the following script:\n#!/bin/bash while read service; do echo \u0026#34;Starting $service...\u0026#34; docker compose -f ./$service/docker-compose.yml up -d done \u0026lt; services.txt with a services.txt file, to easily remove a service if needed later:\nunifi pihole wireguard and added the line to crontab using crontab -e, so that it executes at startup:\n@reboot sh /home/rick/start-services.sh Conclusion Overall, this was a fairly pleasant learning experience. I left out some of the troubles I encountered and details with setting up some of the specifics of each server, but I hope that this helps anyone looking to get their feet wet with Docker.\nCheers!\nRick\nReferences and Resources https://learn.cantrill.io/courses/docker-fundamentals https://github.com/linuxserver/docker-unifi-controller https://github.com/linuxserver/docker-wireguard https://github.com/pi-hole/docker-pi-hole\n","permalink":"https://rickt.io/posts/docker-for-home-services/","summary":"Learning how to use Docker with existing images to host PiHole, Wireguard, and Unifi home services.","title":"Docker for home services"}]